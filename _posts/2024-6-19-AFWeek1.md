---
layout: post
usemathjax: True
title: Axiom Futures AI Safety Week 1 Notes 

---


Excercise solutions:
1. In NNs, weights are what define how much a neuron influences others. 
2. Biases are a constant number added to a neuron's activation.
3. What's the value of A, B and C in this neural network that uses the sigmoid function? A= $\frac{1}{1+e^{3.6}}=0.646$, B=0.42, C = $2*\frac{1}{1+e^0.6} + 1 \times \frac{1}{1+e^(-0.3)}=0.938$
4. The cost function is a measure of how far the current prediction is from the prediction outcome we want to train the NN to have. 
5. It brings the neural network's prediction closer to the prediction outcome we want it to have. 
6. Because the gradient is the direction of steepest ascent, negative of it represents direction of steepest descent. Iteratively choosing this allows us to "find" local minima on the loss function manifold. 
7. Character level encoding, or Subword level encoding or word level encoding. 
8. Pretraining and finetuning. Pretraining is the computationally and monetarily costly step of adjusting the weights for a large corpus (e.g. a significant chunk of the internet) whereas finetuning is using high quality data to update the pretrained model to some specific tasks. 

1. Did decent. 
2. Yeah, we expect it to be good at next word generation task. 
3. LLMs are bad at math without using tools because zero shot reasoning. with chain of thought prompting and using tools they'll perform much better. (Leopold Aschebrenner - system 1 vs system 2 thinking).

