---
title: Axiom Futures AI Safety Week 2
layout: 
usematjax: True
---

## Why are people building AI systems article
[article](https://aisafetyfundamentals.com/blog/why-are-people-building-ai-systems/)
### Automating tasks
McKinsey estimate of 0.1 to 0.6% growth rate in productivity (!?) which leads to a value addition of $2.6 to 4.4 Trillion dollars.
- this is a disputed figure. Original McKinsey article estimates use cases such as hiring which people are quite skeptical about because of problems with fairness e.g. or hallucinations and stuff. 
- earlier AI systems were thought to have good use cases in operations and supply chain but those were more compute/calculation based optimizations which are now not considered as relevant because of generative AI. 
- Customer Operations, Sales, Software Engineering, R&D. (only SWE and R&D seem legit).

### Replacing Human workers
e.g. Devin or the idea of a "drop in" agent as mentioned by Leopold Aschenbrenner

### Non Economic Motivations:
1. Positive externalities of technology development
2. National Interests (proprietary models and when AGI comes its going to be like nuclear power).
3. Fun Stuff/ Solving their own problems. 

## Compute Trends across three eras of ML article

Better ML systems come from: 
1. Better Algorithms. 
2. Better Data.
3. More Compute. 

This article aims to analyse training compute trends. 

### Research Methodology 
How is compute trained:
1. Counting number of operations. 
2. Estimating through Runtime - requires assumptions about GPUs and system used for training - more uncertain. 

Only the "important models" (influence, number of citations) were considered that pushed the SoTA. 

### Results: 
1. Old era doubling time ~20 months. 
2. DL era (~2010-12) doubling time of around 6 months. 
3. Large Model Era (~2016) doubling time of around 10 months. 


## Scaling Laws article
[article](https://blog.finxter.com/ai-scaling-laws-a-short-primer/)
Compute, Data, Parameters.
Neural Language models scale with a consistent power law wrt to these.
Transformer architecture doesn't converge. 


## Paper on AI prediction
High-level machine intelligence (HLMI) is achieved when unaided machines can accomplish every
task better and more cheaply than human workers. Ignore aspects of tasks for which being a human
is intrinsically advantageous, e.g. being accepted as a jury member

### 3.6 Intelligence Explosion
Since 2016 a majority of respondents have thought that it’s either “quite likely,” “likely,” or an “about even chance” that technological progress becomes more than an order of magnitude faster within 5 years of HLMI being achieved

### 3.7 Future Systems 
There were areas of agreement, however. For instance, a large majority of participants thought state-of-the-art AI
systems in twenty years would be likely or very likely to:
1. Find unexpected ways to achieve goals (82.3% of respondents),
2. Be able to talk like a human expert on most topics (81.4% of respondents), and
3. Frequently behave in ways that are surprising to humans (69.1% of respondents)

### Interpretibility/ Explainable AI systems
For typical state-of-the-art AI systems in 2028, do you think it will be possible for users to know the
true reasons for systems making a particular choice? By “true reasons” we mean the AI correctly
explains its internal decision-making process in a way humans can understand. By “true reasons” we
do not mean the decision itself is correct.
Only ~20 percent think that its likely that this is going to be the case. 



### Prediction Excercise

1. Win Putnam Math Contest (score in the top 10 percentile of candidates on the putnam examination). 80 percent confidence interval by 2028
2. Simple Python Code given spec and examples 100 percent confidence interval 2023
3. Finetuneing an LLM (import a model from the web and finetune it given a dataset without external help)
75 percent confidence interval 2028
Reasoning: I have tried doing it currently with good prompting it is almost able to do so. This is why the estimate of the paper as 2028 seems good to me as incremental gains such as RLHF will make it much easier. 

