---
title: Axiom Futures AI Safety Week 2
layout: post 
usematjax: True
---

## Why are people building AI systems article
[article](https://aisafetyfundamentals.com/blog/why-are-people-building-ai-systems/)
### Automating tasks
McKinsey estimate of 0.1 to 0.6% growth rate in productivity (!?) which leads to a value addition of $2.6 to 4.4 Trillion dollars.
- this is a disputed figure. Original McKinsey article estimates use cases such as hiring which people are quite skeptical about because of problems with fairness e.g. or hallucinations and stuff. 
- earlier AI systems were thought to have good use cases in operations and supply chain but those were more compute/calculation based optimizations which are now not considered as relevant because of generative AI. 
- Customer Operations, Sales, Software Engineering, R&D. (only SWE and R&D seem legit).

### Replacing Human workers
e.g. Devin or the idea of a "drop in" agent as mentioned by Leopold Aschenbrenner

### Non Economic Motivations:
1. Positive externalities of technology development
2. National Interests (proprietary models and when AGI comes its going to be like nuclear power).
3. Fun Stuff/ Solving their own problems. 

## Compute Trends across three eras of ML article

Better ML systems come from: 
1. Better Algorithms. 
2. Better Data.
3. More Compute. 

This article aims to analyse training compute trends. 

### Research Methodology 
How is compute trained:
1. Counting number of operations. 
2. Estimating through Runtime - requires assumptions about GPUs and system used for training - more uncertain. 

Only the "important models" (influence, number of citations) were considered that pushed the SoTA. 

### Results: 
1. Old era doubling time ~20 months. 
2. DL era (~2010-12) doubling time of around 6 months. 
3. Large Model Era (~2016) doubling time of around 10 months. 


## Scaling Laws article
[article](https://blog.finxter.com/ai-scaling-laws-a-short-primer/)
Compute, Data, Parameters.
Neural Language models scale with a consistent power law wrt to these.
Transformer architecture doesn't converge. 


## Paper on AI prediction
High-level machine intelligence (HLMI) is achieved when unaided machines can accomplish every
task better and more cheaply than human workers. Ignore aspects of tasks for which being a human
is intrinsically advantageous, e.g. being accepted as a jury member

### 3.6 Intelligence Explosion
Since 2016 a majority of respondents have thought that it’s either “quite likely,” “likely,” or an “about even chance” that technological progress becomes more than an order of magnitude faster within 5 years of HLMI being achieved

### 3.7 Future Systems 
There were areas of agreement, however. For instance, a large majority of participants thought state-of-the-art AI
systems in twenty years would be likely or very likely to:
1. Find unexpected ways to achieve goals (82.3% of respondents),
2. Be able to talk like a human expert on most topics (81.4% of respondents), and
3. Frequently behave in ways that are surprising to humans (69.1% of respondents)

### Interpretibility/ Explainable AI systems
For typical state-of-the-art AI systems in 2028, do you think it will be possible for users to know the
true reasons for systems making a particular choice? By “true reasons” we mean the AI correctly
explains its internal decision-making process in a way humans can understand. By “true reasons” we
do not mean the decision itself is correct.
Only ~20 percent think that its likely that this is going to be the case. 



### Prediction Excercise

My Predictions:
1. Event: Win Putnam Math Contest (score in the top 10 percentile of candidates on the putnam examination). 
Prediction: 75 percent confidence interval by 2027
Reasoning: We have already done pretty decent on IMO benchmarks and putnam is pretty similar so I think it is reasonable to expect this.

2. Event: Simple Python Code given spec and examples 
Prediction: 100 percent confidence interval 2023 
Reasoning: (Claude 3 opus and GPT-4 pretty much solve this).

3. Event: Finetuneing an LLM (import a model from the web and finetune it given a dataset without external help)
Prediction: 75 percent confidence interval 2028
Reasoning: I have tried doing it currently with good prompting it is almost able to do so. This is why the estimate of the paper as 2028 seems good to me as incremental gains such as RLHF will make it much easier. 

Partner Predictions:
1. Vid from diff angle.
prediction: 75 percent confidence by 2028 
reasoning: 2020 panoptic scene graph - semantic ideas of images. SoRA


## How will AI affect your job Excercise:
1. Reading research papers or textbooks, coding, solving physics problems, interacting with others to see progress and get ideas for my own research. 
Comprehension of Research papers/textbooks - pretty surface level understanding of the concepts, especially for deeper material that requires more prerequisites. 
Coding - good for boilerplate code, specific hard problems is difficult to solve. I use it often for generating boilerplate code with bare functionalities and then debugging. 
Solving Physics problems - Not very good. Linked to the surface level comprehension of ideas. 
Human interaction: Terrible. AI generated mails are a chore to read through. 
2. in 10 years:
Comprehension: Better RLHF etc. has led to a deeper understanding of concepts especially when prompted well. 
Coding: Significantly less bugs, Privacy concerns or Hallucinations have probably not been sorted out but the adoption transition has started and SWE jobs become less overpaid. 
Solving Research Level Problems in the Sciences: Good progress may be made to the point where we could be using these as pair researchers however I do not think science research would have been totally automated by then.
Human interaction: a significant portion of human interaction especially via text and speech modalities is AI generated. 

3. Technology adoption would be quite slow. Even with all the developments there would be a significant transition delay. With superintelligent AI research systems I would say almost all my present work could be automated.  

4. AI would exacerbate the rockstar effect considerably. But it would also empower creatives. At the same time it would also lead to content saturation on the internet which would lead to smaller communities that act as patreon for creatives.  

Partner ideas: 
1. Repetitive jobs will be outsourced e.g. desk jobs. 
2. Transformer Models are data hungry but also can't really do "reasoning tasks".
- Transformers don't learn data itself, they form a meta-model. 
- tooling will get better. 
- We already do better on IMO 

Question for the week: Who is liable for the damages if an AI system generates something that wouldn;t have been possible otherwise. 

What improvements can you do to a superintelligent AI system that it can't do for itself in the near future. 
